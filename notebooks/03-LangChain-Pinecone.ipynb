{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa52152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21e3831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AAAAAAA\\RAG\\Projects\\LangChain - Basics, Chat and RAG\\LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "# listing all available models\n",
    "import google.generativeai as genai\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3057aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda63c8",
   "metadata": {},
   "source": [
    "* `Connect Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a3ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"quickstart\"\n",
    "\n",
    "try:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Google embedding-001 uses 768 dimensions\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c601066",
   "metadata": {},
   "source": [
    "* `Splitting & Embedding Text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0489ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks is 300\n",
      "From the moment that the French defenses at Sedan and on the Meuse were broken at the end of the\n"
     ]
    }
   ],
   "source": [
    "# Read Churchill speech\n",
    "try:\n",
    "    with open(\"../assets/churchill_speech.txt\") as f:\n",
    "        texts = f.read()\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"churchill_speech.txt not found in ../assets/. Please verify the file path.\")\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.create_documents([texts])\n",
    "\n",
    "print(f'Number of Chunks is {len(chunks)}')\n",
    "print(chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36298700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens is: 4820\n",
      "Approximate token count for cost estimation: 4820\n"
     ]
    }
   ],
   "source": [
    "# Calculate embedding cost (approximate, as Google pricing differs)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # Use a general encoding for token counting\n",
    "total_tokens = sum([len(enc.encode(i.page_content)) for i in chunks])\n",
    "\n",
    "print(f'Total Tokens is: {total_tokens}')\n",
    "# Note: Google embedding pricing varies; check Google Cloud pricing for exact cost\n",
    "print(f'Approximate token count for cost estimation: {total_tokens}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0da4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding successful. First 10 values: [0.033345527946949005, -0.002517310669645667, -0.02232394367456436, -0.02673177421092987, 0.0484866239130497, 0.03872605413198471, 0.04826189577579498, -0.02397942915558815, 0.030039627104997635, 0.07229742407798767]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google embeddings with updated model\n",
    "try:\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",  # Updated model name\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to initialize GoogleGenerativeAIEmbeddings: {str(e)}\")\n",
    "\n",
    "# Test embedding\n",
    "try:\n",
    "    vector_0 = embeddings.embed_query(chunks[0].page_content)\n",
    "    print(\"Embedding successful. First 10 values:\", vector_0[:10])\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Embedding failed: {str(e)}. Please verify your Google API key and Vertex AI API setup at https://console.cloud.google.com/apis/credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357241d",
   "metadata": {},
   "source": [
    "* `Upserting to Pincone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d698ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully upserted to Pinecone vector store.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name,\n",
    "        pinecone_api_key=PINECONE_API_KEY\n",
    "    )\n",
    "    print(\"Successfully upserted to Pinecone vector store.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to upsert to Pinecone: {str(e)}. Please verify your Pinecone API key and index setup at https://app.pinecone.io.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2279e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9db11a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 900}},\n",
       " 'total_vector_count': 900,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check after\n",
    "index.describe_index_stats() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3abbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing\n",
      "--------------------------------------------------\n",
      "end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing\n",
      "--------------------------------------------------\n",
      "end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = 'Where should we fight?'\n",
    "result = vector_store.similarity_search(query=query, k=3)\n",
    "\n",
    "for r in result:\n",
    "    print(r.page_content)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129ff61",
   "metadata": {},
   "source": [
    "* `Answering using LLM (RAG)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb0569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\" , api_key=GOOGLE_API_KEY , temperature=0.2)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type = \"similarity\" , search_kwargs={'k': 4})\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm = llm , chain_type=\"stuff\" , retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d200c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France, the seas, and the oceans.\n"
     ]
    }
   ],
   "source": [
    "# An example\n",
    "query = 'Answer only from the provided input. Where should we fight?'\n",
    "answer = chain.invoke(query)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66bd91ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text mentions the King of the Belgians, but doesn't state his name.\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "query = 'Answer only from the provided input. Who was the king of Belgium at that time?'\n",
    "answer = chain.invoke(query)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ca8ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The French defenses at Sedan and on the Meuse were broken.\n"
     ]
    }
   ],
   "source": [
    "# Another exmaple\n",
    "query = 'Answer only from the provided input. Does French defenses at Sedan?'\n",
    "answer = chain.invoke(query)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d976f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
